{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üöÄ Forex Trading System - PPO Training (SIMPLIFIED VERSION)\n\n**Goal**: Train AI model with realistic 3% risk strategy for profitable forex trading\n\n**Strategy**:\n- ‚úÖ **SIMPLIFIED**: Single-pair trading (USD/JPY only)\n- ‚úÖ 3% risk per trade with 1:1.5 reward ratio\n- ‚úÖ $10,000 account, dynamic position sizing\n- ‚úÖ PPO reinforcement learning with 200k step checkpoints\n- ‚úÖ Immediate reward feedback to encourage trading\n\n**Expected Training Time**: 12-24 hours total (200k steps per session)\n\n**Key Improvement**: Simplified to single pair to make learning easier and see trades happening!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install stable-baselines3==2.1.0\n!pip install sb3-contrib==2.1.0  # For RecurrentPPO with LSTM\n!pip install gymnasium==0.29.1\n!pip install ta-lib==0.4.28\n!pip install torch torchvision torchaudio\n!pip install tensorboard\n!pip install loguru\n!pip install PyYAML\n!pip install pandas numpy matplotlib scikit-learn\n!pip install quantstats\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"‚úÖ All dependencies installed!\")\nprint(\"üß† LSTM-based RecurrentPPO ready for temporal learning\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU and system info\n",
    "import torch\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "print(\"üéÆ System Information:\")\n",
    "print(f\"   Current directory: {os.getcwd()}\")\n",
    "print(f\"   Available RAM: {psutil.virtual_memory().available / 1e9:.1f} GB\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"‚úÖ GPU Available: {gpu_name}\")\n",
    "    print(f\"   GPU Memory: {gpu_memory:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU available - training will be extremely slow!\")\n",
    "    print(\"   Go to Runtime ‚Üí Change Runtime Type ‚Üí Hardware Accelerator ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## üìÅ Create Project Structure and Upload Files\n\n**Required Files to Upload:**\n1. `src/data_manager.py` - Data processing and indicators\n2. `src/environment_single.py` - **SIMPLIFIED** Single-pair trading environment\n3. `training/train_ppo.py` - PPO training script\n4. `config/training.yaml` - Training configuration\n5. `requirements_colab.txt` - Additional requirements\n6. **Optional**: Your CSV file: `USDJPY_M15.csv` (only need one pair now!)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create required directories\nimport os\n\ndirs = [\n    'src', 'training', 'config', 'data', \n    'logs', 'logs/tensorboard', 'models', \n    'models/checkpoints', 'models/final'\n]\n\nfor dir_name in dirs:\n    os.makedirs(dir_name, exist_ok=True)\n    print(f\"üìÅ Created directory: {dir_name}\")\n\nprint(\"\\nüìã Upload your project files to these locations:\")\nprint(\"   ‚Ä¢ data_manager.py ‚Üí /content/src/\")\nprint(\"   ‚Ä¢ environment_single.py ‚Üí /content/src/\")  # CHANGED\nprint(\"   ‚Ä¢ train_ppo.py ‚Üí /content/training/\")\nprint(\"   ‚Ä¢ training.yaml ‚Üí /content/config/\")\nprint(\"   ‚Ä¢ requirements_colab.txt ‚Üí /content/\" )\nprint(\"   ‚Ä¢ USDJPY_M15.csv (optional) ‚Üí /content/data/\")  # CHANGED\n\nprint(\"\\n‚ö†Ô∏è IMPORTANT: Make sure all files are uploaded before proceeding!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional requirements if file exists\n",
    "if os.path.exists('/content/requirements_colab.txt'):\n",
    "    print(\"üì¶ Installing additional requirements...\")\n",
    "    !pip install -r requirements_colab.txt\n",
    "    print(\"‚úÖ Additional requirements installed\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No requirements_colab.txt found - using base packages only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify file uploads\nrequired_files = {\n    'src/data_manager.py': 'Data processing module',\n    'src/environment_single.py': 'Single-pair trading environment',  # CHANGED\n    'training/train_ppo.py': 'PPO training script',\n    'config/training.yaml': 'Training configuration'\n}\n\nprint(\"üîç Checking required files:\")\nall_files_present = True\n\nfor file_path, description in required_files.items():\n    if os.path.exists(file_path):\n        size_kb = os.path.getsize(file_path) / 1000\n        print(f\"‚úÖ {file_path} - {description} ({size_kb:.1f} KB)\")\n    else:\n        print(f\"‚ùå {file_path} - {description} - FILE MISSING!\")\n        all_files_present = False\n\n# Check optional CSV file (CHANGED - only USDJPY now)\ncsv_file = 'data/USDJPY_M15.csv'\nprint(\"\\nüîç Checking optional CSV file:\")\nif os.path.exists(csv_file):\n    size_mb = os.path.getsize(csv_file) / 1e6\n    print(f\"‚úÖ {csv_file} - USD/JPY data ({size_mb:.1f} MB)\")\n    csv_available = True\nelse:\n    print(f\"‚ÑπÔ∏è {csv_file} - Not uploaded (will use sample data)\")\n    csv_available = False\n\nprint(f\"\\nüìä Summary:\")\nprint(f\"   Required files: {'‚úÖ All present' if all_files_present else '‚ùå Missing files'}\")\nprint(f\"   CSV file: {'‚úÖ Uploaded' if csv_available else '‚ÑπÔ∏è Using sample data'}\")\n\nif not all_files_present:\n    print(\"\\n‚ö†Ô∏è STOP: Upload missing files before continuing!\")\nelse:\n    print(\"\\nüéØ Ready to proceed with training!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data loading\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Add paths\n",
    "sys.path.append('/content/src')\n",
    "sys.path.append('/content/training')\n",
    "\n",
    "print(\"üìä Testing data pipeline...\")\n",
    "\n",
    "try:\n",
    "    from data_manager import DataManager\n",
    "    print(\"‚úÖ DataManager imported successfully\")\n",
    "    \n",
    "    # Initialize DataManager\n",
    "    dm = DataManager()\n",
    "    print(\"‚úÖ DataManager initialized\")\n",
    "    \n",
    "    # Check for CSV files\n",
    "    csv_files = {\n",
    "        'EURUSD': '/content/data/EURUSD_M15.csv',\n",
    "        'AUDCHF': '/content/data/AUDCHF_M15.csv', \n",
    "        'USDJPY': '/content/data/USDJPY_M15.csv'\n",
    "    }\n",
    "    \n",
    "    available_csvs = {}\n",
    "    for pair, file_path in csv_files.items():\n",
    "        if os.path.exists(file_path):\n",
    "            available_csvs[pair] = file_path\n",
    "    \n",
    "    if available_csvs:\n",
    "        print(f\"\\nüìà Loading {len(available_csvs)} CSV files...\")\n",
    "        try:\n",
    "            # Load CSV data using DataManager\n",
    "            data = dm.load_csv_data(available_csvs)\n",
    "            print(f\"‚úÖ Real CSV data loaded for {len(data)} pairs!\")\n",
    "            use_real_data = True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è CSV loading failed: {e}\")\n",
    "            print(\"üìä Falling back to sample data...\")\n",
    "            data = dm.get_multi_pair_data()\n",
    "            use_real_data = False\n",
    "    else:\n",
    "        print(\"üìä No CSV files found - using sample data...\")\n",
    "        data = dm.get_multi_pair_data()\n",
    "        use_real_data = False\n",
    "    \n",
    "    # Display data summary\n",
    "    print(f\"\\nüìä Data Summary ({'Real CSV' if use_real_data else 'Sample Data'}):\")\n",
    "    total_candles = 0\n",
    "    for pair, df in data.items():\n",
    "        features = len([col for col in df.columns if col not in ['Symbol']])\n",
    "        candles = len(df)\n",
    "        total_candles += candles\n",
    "        date_range = f\"{df.index[0]} to {df.index[-1]}\" if hasattr(df.index, 'min') else \"N/A\"\n",
    "        print(f\"   {pair}: {candles:,} candles, {features} features ({date_range})\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total: {total_candles:,} candles across {len(data)} pairs\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Data pipeline test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nüîß Make sure all required files are uploaded correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Test Trading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test trading environment\nprint(\"üèóÔ∏è Testing single-pair trading environment...\")\n\ntry:\n    from environment_single import SinglePairForexEnv  # CHANGED\n    print(\"‚úÖ Environment module imported\")\n\n    # Create environment with available data\n    if 'data' in locals():\n        # Get USDJPY data only\n        usdjpy_data = data.get('USDJPY', list(data.values())[0])\n        \n        env = SinglePairForexEnv(usdjpy_data, initial_balance=10000.0)\n        print(\"‚úÖ Single-pair environment created with USD/JPY data\")\n\n        # Display environment info\n        print(f\"   Currency pair: USD/JPY\")\n        print(f\"   Action space: {env.action_space}\")\n        print(f\"   Actions: 3 (HOLD, BUY, SELL)\")\n        print(f\"   Observation space: {env.observation_space.shape}\")\n        print(f\"   Initial balance: ${env.initial_balance:,}\")\n        print(f\"   Risk per trade: {env.risk_per_trade_pct * 100}%\")\n\n        # Quick environment test\n        obs, info = env.reset()\n        print(f\"\\nüß™ Environment test:\")\n        print(f\"   Observation shape: {obs.shape}\")\n        print(f\"   Initial balance: ${info['balance']:.2f}\")\n\n        # Test a few random actions\n        for i in range(5):\n            action = env.action_space.sample()  # Returns 0, 1, or 2\n            obs, reward, done, truncated, info = env.step(action)\n\n            action_name = ['HOLD', 'BUY', 'SELL'][action]\n            print(f\"   Step {i+1}: {action_name}, Reward: {reward:.1f}, Balance: ${info['balance']:.2f}, Has position: {info['has_position']}\")\n\n            if done:\n                break\n\n        print(\"\\n‚úÖ Environment test completed successfully!\")\n        print(f\"   The agent is {('TRADING!' if env.episode_stats['total_trades'] > 0 else 'not trading yet')}\")\n\n    else:\n        print(\"‚ùå No data available for environment test\")\n\nexcept Exception as e:\n    print(f\"‚ùå Environment test failed: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Start PPO Training\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT**: \n",
    "- Training runs in 200k step chunks (2-4 hours each)\n",
    "- Total target: 500k steps (3 sessions max)\n",
    "- Auto-saves checkpoints to handle Colab quotas\n",
    "- Do NOT close browser during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Start or resume PPO training\nprint(\"üöÄ STARTING PPO TRAINING WITH LSTM POLICY\")\nprint(\"=\" * 60)\nprint(\"üß† Architecture: RecurrentPPO with LSTM (temporal learning)\")\nprint(\"üìä Reward: Sharpe ratio-based (risk-adjusted)\")\nprint(\"üí∞ Position sizing: True 3% risk ($300 per trade)\")\nprint(\"‚ö° Improvements: Transaction costs in reward, slippage, 30% max drawdown\")\nprint(\"\")\nprint(\"üéâ Training on:\", 'Your CSV files' if use_real_data else 'Sample data')\nprint(\"\")\nprint(\"=\" * 60)\n\ntry:\n    # Import training module\n    from train_ppo import ForexTrainer, find_latest_checkpoint\n    print(\"‚úÖ Training module imported\")\n    \n    # Check for existing checkpoints\n    latest_checkpoint = find_latest_checkpoint()\n    \n    if latest_checkpoint:\n        print(f\"\\nüîç Found existing checkpoint: {os.path.basename(latest_checkpoint)}\")\n        print(\"üìà Resuming training from previous session...\")\n        resume_training = True\n    else:\n        print(\"\\nüÜï Starting fresh training session...\")\n        resume_training = False\n    \n    # Initialize trainer\n    trainer = ForexTrainer()\n    print(\"‚úÖ Trainer initialized\")\n    \n    # Start training with appropriate data\n    if use_real_data and available_csvs:\n        print(f\"üìà Training with {len(available_csvs)} CSV files...\")\n        if resume_training:\n            trainer.train(csv_files=available_csvs, resume_from=latest_checkpoint)\n        else:\n            trainer.train(csv_files=available_csvs)\n    else:\n        print(\"üìä Training with sample data...\")\n        if resume_training:\n            trainer.train(resume_from=latest_checkpoint)\n        else:\n            trainer.train()\n    \n    print(\"\\nüéâ Training session completed successfully!\")\n    print(\"\\nüì• IMPORTANT - Download these files:\")\n    print(\"   üìÅ /content/models/checkpoints/ - All checkpoint files\")\n    print(\"   üìÅ /content/models/final/ - Final model (if training complete)\")\n    print(\"   üìÅ /content/logs/ - Training logs and analytics\")\n    print(\"\")\n    print(\"üîÑ For next session: Upload checkpoint files back to Colab to resume!\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Training failed: {e}\")\n    import traceback\n    traceback.print_exc()\n    \n    # Check for emergency checkpoints\n    emergency_files = []\n    if os.path.exists('/content/models/checkpoints'):\n        for file in os.listdir('/content/models/checkpoints'):\n            if 'emergency' in file.lower():\n                emergency_files.append(file)\n    \n    if emergency_files:\n        print(f\"\\nüíæ Emergency checkpoints available:\")\n        for file in emergency_files:\n            print(f\"   üìÑ {file}\")\n        print(\"üì• Download these files to resume training later!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Monitor Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard for monitoring\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/logs/tensorboard --host 0.0.0.0 --port 6006\n",
    "\n",
    "print(\"üìà TensorBoard loaded!\")\n",
    "print(\"\\nüìä Key metrics to monitor:\")\n",
    "print(\"   ‚Ä¢ ep_rew_mean: Episode rewards (should trend upward)\")\n",
    "print(\"   ‚Ä¢ win_rate: Trade win rate (target >60%)\")\n",
    "print(\"   ‚Ä¢ avg_profit: Average profit per trade (target >$20)\")\n",
    "print(\"   ‚Ä¢ total_trades: Number of trades executed\")\n",
    "print(\"   ‚Ä¢ balance_progression: Account balance over time\")\n",
    "print(\"\")\n",
    "print(\"üéØ Good signs:\")\n",
    "print(\"   ‚úÖ Rewards trending from negative to positive\")\n",
    "print(\"   ‚úÖ Win rate stabilizing above 60%\")\n",
    "print(\"   ‚úÖ Consistent trade execution (not all HOLD actions)\")\n",
    "print(\"   ‚úÖ Balance growing steadily over episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Resume Training (Day 2+)\n",
    "\n",
    "**Instructions for resuming training:**\n",
    "1. Upload checkpoint files to `/content/models/checkpoints/`\n",
    "2. Run the cell below to continue training\n",
    "3. Training will automatically detect and resume from latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training from uploaded checkpoint\n",
    "print(\"üîÑ RESUME TRAINING SYSTEM\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check for uploaded checkpoint files\n",
    "checkpoint_dir = '/content/models/checkpoints'\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.startswith('forex_ppo_chunk_')]\n",
    "    \n",
    "    if checkpoint_files:\n",
    "        print(f\"‚úÖ Found {len(checkpoint_files)} checkpoint files:\")\n",
    "        for file in sorted(checkpoint_files):\n",
    "            file_path = os.path.join(checkpoint_dir, file)\n",
    "            size_mb = os.path.getsize(file_path) / 1e6\n",
    "            print(f\"   üìÑ {file} ({size_mb:.1f} MB)\")\n",
    "        \n",
    "        # Find latest checkpoint\n",
    "        latest_file = max(checkpoint_files, key=lambda x: os.path.getmtime(os.path.join(checkpoint_dir, x)))\n",
    "        latest_path = os.path.join(checkpoint_dir, latest_file)\n",
    "        print(f\"\\nüîç Latest checkpoint: {latest_file}\")\n",
    "        \n",
    "        # Resume training\n",
    "        print(\"\\nüöÄ Resuming training...\")\n",
    "        try:\n",
    "            from train_ppo import ForexTrainer\n",
    "            \n",
    "            trainer = ForexTrainer()\n",
    "            \n",
    "            # Resume with appropriate data\n",
    "            if 'available_csvs' in locals() and available_csvs:\n",
    "                trainer.train(csv_files=available_csvs, resume_from=latest_path)\n",
    "            else:\n",
    "                trainer.train(resume_from=latest_path)\n",
    "            \n",
    "            print(\"\\n‚úÖ Training session completed!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Resume training failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ùå No checkpoint files found in checkpoints directory!\")\n",
    "        print(\"üì§ Please upload your checkpoint files first.\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Checkpoints directory not found!\")\n",
    "    print(\"üìÅ Creating directory - please upload checkpoint files.\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# TensorBoard Visualization (for detailed analysis)\n%load_ext tensorboard\n%tensorboard --logdir /content/logs/tensorboard --host 0.0.0.0 --port 6006\n\nprint(\"üìà TensorBoard loaded!\")\nprint(\"\\nüîç KEY METRICS TO CHECK IN TENSORBOARD:\")\nprint(\"\\n1. SCALARS TAB - Look for these metrics:\")\nprint(\"   üìä forex/win_rate - Should trend upward to >50%\")\nprint(\"   üí∞ forex/profit_factor - Target: >1.5\")\nprint(\"   üìâ forex/max_drawdown - Should stay <30%\")\nprint(\"   üéØ eval/mean_reward - Should become positive\")\nprint(\"   üìà train/policy_loss - Should decrease\")\nprint(\"   üìâ train/value_loss - Should stabilize\")\nprint(\"   üß† train/explained_variance - Target: >0.8\")\nprint(\"\\n2. LOOK FOR TRENDS:\")\nprint(\"   ‚úÖ GOOD: Metrics improving steadily over time\")\nprint(\"   ‚ö†Ô∏è  PLATEAU: Metrics flat for 100k+ steps\")\nprint(\"   ‚ùå BAD: Metrics degrading or very unstable\")\nprint(\"\\n3. DECISION GUIDE:\")\nprint(\"   ‚Ä¢ Trending up ‚Üí Continue training\")\nprint(\"   ‚Ä¢ Plateau at good values ‚Üí Start evaluation\")\nprint(\"   ‚Ä¢ Plateau at poor values ‚Üí Adjust hyperparameters\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Quick Metrics Check from Latest Metadata File\nimport yaml\nimport os\nimport glob\n\nprint(\"üìä FINAL TRAINING METRICS CHECK\")\nprint(\"=\" * 60)\n\n# Find all metadata files\ncheckpoint_dir = '/content/models/checkpoints'\nmetadata_files = glob.glob(f'{checkpoint_dir}/*metadata*.yaml')\n\nif metadata_files:\n    # Get the most recent metadata file (excluding emergency)\n    regular_metadata = [f for f in metadata_files if 'emergency' not in f]\n    \n    if regular_metadata:\n        latest_metadata = max(regular_metadata, key=os.path.getmtime)\n        \n        with open(latest_metadata, 'r') as f:\n            metadata = yaml.safe_load(f)\n        \n        print(f\"‚úÖ Loaded: {os.path.basename(latest_metadata)}\")\n        print(f\"\\nüìà PERFORMANCE METRICS:\")\n        print(f\"   Win Rate: {metadata.get('win_rate', 'N/A')}%\")\n        print(f\"   Profit Factor: {metadata.get('profit_factor', 'N/A')}\")\n        print(f\"   Max Drawdown: {metadata.get('max_drawdown', 'N/A')}%\")\n        print(f\"   Total Trades: {metadata.get('total_trades', 'N/A')}\")\n        print(f\"   Goal Trades: {metadata.get('total_goal_trades', 'N/A')}\")\n        print(f\"   Total Steps: {metadata.get('timesteps', 'N/A'):,}\")\n        \n        print(f\"\\nüéØ TARGET COMPARISON:\")\n        win_rate = metadata.get('win_rate', 0)\n        pf = metadata.get('profit_factor', 0)\n        dd = metadata.get('max_drawdown', 100)\n        \n        win_status = \"‚úÖ\" if win_rate >= 50 else \"‚è≥\" if win_rate >= 45 else \"‚ùå\"\n        pf_status = \"‚úÖ\" if pf >= 1.5 else \"‚è≥\" if pf >= 1.2 else \"‚ùå\"\n        dd_status = \"‚úÖ\" if dd <= 30 else \"‚ö†Ô∏è\"\n        \n        print(f\"   {win_status} Win Rate: {win_rate}% (Target: >50%)\")\n        print(f\"   {pf_status} Profit Factor: {pf} (Target: >1.5)\")\n        print(f\"   {dd_status} Max Drawdown: {dd}% (Target: <30%)\")\n        \n        print(f\"\\nüìä RECOMMENDATION:\")\n        if win_rate >= 50 and pf >= 1.5 and dd <= 30:\n            print(\"   üéâ ALL TARGETS MET!\")\n            print(\"   ‚Üí Start model evaluation and backtesting\")\n        elif win_rate >= 45 and pf >= 1.2:\n            print(\"   ‚è≥ CLOSE TO TARGETS!\")\n            print(\"   ‚Üí Consider 1-2 more training chunks (200k-400k steps)\")\n        else:\n            print(\"   üîÑ NEEDS MORE TRAINING\")\n            print(\"   ‚Üí Continue for 2-4 more chunks or review hyperparameters\")\n    else:\n        print(\"‚ùå Only emergency metadata found - upload regular checkpoint files\")\nelse:\n    print(\"‚ùå No metadata files found!\")\n    print(\"üì§ Upload your checkpoint files to check metrics\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üìä Check Training Metrics\n\n**Use this section to evaluate your completed training before deciding whether to:**\n- ‚úÖ Start evaluation (if targets met)\n- ‚è≥ Continue training (if close to targets)\n- üîÑ Adjust hyperparameters (if not improving)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Training Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze training results\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üìä TRAINING RESULTS ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check training progress\n",
    "checkpoint_files = glob.glob('/content/models/checkpoints/forex_ppo_chunk_*')\n",
    "if checkpoint_files:\n",
    "    # Extract chunk numbers to determine progress\n",
    "    chunk_numbers = []\n",
    "    for file_path in checkpoint_files:\n",
    "        try:\n",
    "            filename = os.path.basename(file_path)\n",
    "            # Extract chunk number from filename like 'forex_ppo_chunk_1_20231125_143022'\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) >= 4:\n",
    "                chunk_num = int(parts[3])\n",
    "                chunk_numbers.append(chunk_num)\n",
    "        except (ValueError, IndexError):\n",
    "            continue\n",
    "    \n",
    "    if chunk_numbers:\n",
    "        max_chunk = max(chunk_numbers)\n",
    "        total_steps = max_chunk * 200000  # 200k steps per chunk\n",
    "        progress_percent = min(total_steps / 500000 * 100, 100)  # Target 500k steps\n",
    "        \n",
    "        print(f\"‚úÖ Training Progress:\")\n",
    "        print(f\"   Completed chunks: {max_chunk}/3\")\n",
    "        print(f\"   Total steps: {total_steps:,}/500,000\")\n",
    "        print(f\"   Progress: {progress_percent:.1f}%\")\n",
    "        print(f\"   Checkpoint files: {len(checkpoint_files)}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Could not parse checkpoint progress\")\n",
    "else:\n",
    "    print(\"‚ùå No checkpoint files found\")\n",
    "\n",
    "# Check for final model\n",
    "final_model_path = '/content/models/final/forex_ppo_model.zip'\n",
    "if os.path.exists(final_model_path):\n",
    "    size_mb = os.path.getsize(final_model_path) / 1e6\n",
    "    print(f\"\\nüéâ TRAINING COMPLETE!\")\n",
    "    print(f\"   Final model: {size_mb:.1f} MB\")\n",
    "    print(f\"   Ready for download and deployment!\")\n",
    "elif chunk_numbers and max(chunk_numbers) >= 3:\n",
    "    print(f\"\\n‚úÖ Training target reached (3 chunks completed)\")\n",
    "    print(f\"   Check /content/models/checkpoints/ for final model\")\n",
    "else:\n",
    "    remaining = 3 - (max(chunk_numbers) if chunk_numbers else 0)\n",
    "    print(f\"\\n‚è≥ Training in progress\")\n",
    "    print(f\"   Estimated {remaining} more sessions needed\")\n",
    "\n",
    "# Check analytics files\n",
    "analytics_files = glob.glob('/content/logs/*analytics*.csv')\n",
    "if analytics_files:\n",
    "    print(f\"\\nüìà Analytics files available: {len(analytics_files)}\")\n",
    "    for file_path in analytics_files[-3:]:  # Show last 3\n",
    "        filename = os.path.basename(file_path)\n",
    "        size_kb = os.path.getsize(file_path) / 1000\n",
    "        print(f\"   üìÑ {filename} ({size_kb:.1f} KB)\")\n",
    "\n",
    "# Check TensorBoard logs\n",
    "tb_dir = '/content/logs/tensorboard'\n",
    "if os.path.exists(tb_dir) and os.listdir(tb_dir):\n",
    "    tb_files = len([f for f in os.listdir(tb_dir) if os.path.isfile(os.path.join(tb_dir, f))])\n",
    "    print(f\"\\nüìä TensorBoard logs: {tb_files} files\")\n",
    "    print(f\"   Available for performance analysis\")\n",
    "\n",
    "print(f\"\\nüìÖ Analysis completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Download Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create downloadable package with all training results\n",
    "import zipfile\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "def create_training_package():\n",
    "    \"\"\"Create comprehensive download package.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    package_name = f\"forex_training_results_{timestamp}.zip\"\n",
    "    \n",
    "    print(f\"üì¶ Creating training package: {package_name}\")\n",
    "    \n",
    "    with zipfile.ZipFile(package_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        files_added = 0\n",
    "        \n",
    "        # Add checkpoint files\n",
    "        checkpoint_files = glob.glob('/content/models/checkpoints/forex_ppo_chunk_*')\n",
    "        for file_path in checkpoint_files:\n",
    "            arcname = f\"checkpoints/{os.path.basename(file_path)}\"\n",
    "            zipf.write(file_path, arcname)\n",
    "            files_added += 1\n",
    "        print(f\"   ‚úÖ Added {len(checkpoint_files)} checkpoint files\")\n",
    "        \n",
    "        # Add metadata files\n",
    "        metadata_files = glob.glob('/content/models/checkpoints/*metadata*.yaml')\n",
    "        for file_path in metadata_files:\n",
    "            arcname = f\"checkpoints/{os.path.basename(file_path)}\"\n",
    "            zipf.write(file_path, arcname)\n",
    "            files_added += 1\n",
    "        print(f\"   ‚úÖ Added {len(metadata_files)} metadata files\")\n",
    "        \n",
    "        # Add final model if exists\n",
    "        final_model = '/content/models/final/forex_ppo_model.zip'\n",
    "        if os.path.exists(final_model):\n",
    "            zipf.write(final_model, \"final_model/forex_ppo_model.zip\")\n",
    "            files_added += 1\n",
    "            print(\"   ‚úÖ Added final model\")\n",
    "        \n",
    "        # Add analytics files\n",
    "        analytics_files = glob.glob('/content/logs/*analytics*.csv')\n",
    "        for file_path in analytics_files:\n",
    "            arcname = f\"analytics/{os.path.basename(file_path)}\"\n",
    "            zipf.write(file_path, arcname)\n",
    "            files_added += 1\n",
    "        print(f\"   ‚úÖ Added {len(analytics_files)} analytics files\")\n",
    "        \n",
    "        # Add configuration files\n",
    "        config_files = ['/content/config/training.yaml']\n",
    "        for file_path in config_files:\n",
    "            if os.path.exists(file_path):\n",
    "                arcname = f\"config/{os.path.basename(file_path)}\"\n",
    "                zipf.write(file_path, arcname)\n",
    "                files_added += 1\n",
    "        print(f\"   ‚úÖ Added configuration files\")\n",
    "        \n",
    "        # Add recent TensorBoard logs (sample)\n",
    "        tb_files = glob.glob('/content/logs/tensorboard/**/*', recursive=True)\n",
    "        tb_added = 0\n",
    "        for file_path in tb_files[:20]:  # Limit to 20 most recent files\n",
    "            if os.path.isfile(file_path):\n",
    "                rel_path = os.path.relpath(file_path, '/content/logs')\n",
    "                zipf.write(file_path, f\"logs/{rel_path}\")\n",
    "                tb_added += 1\n",
    "        print(f\"   ‚úÖ Added {tb_added} TensorBoard log files\")\n",
    "        \n",
    "        files_added += tb_added\n",
    "    \n",
    "    return package_name, files_added\n",
    "\n",
    "# Create and download package\n",
    "try:\n",
    "    package_file, total_files = create_training_package()\n",
    "    package_size_mb = os.path.getsize(package_file) / 1e6\n",
    "    \n",
    "    print(f\"\\nüìä Package Summary:\")\n",
    "    print(f\"   üì¶ File: {package_file}\")\n",
    "    print(f\"   üìè Size: {package_size_mb:.1f} MB\")\n",
    "    print(f\"   üìÑ Files: {total_files} total\")\n",
    "    \n",
    "    # Auto-download\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        print(f\"\\n‚¨áÔ∏è Starting automatic download...\")\n",
    "        files.download(package_file)\n",
    "        print(f\"‚úÖ Download initiated!\")\n",
    "    except Exception as download_error:\n",
    "        print(f\"‚ö†Ô∏è Auto-download failed: {download_error}\")\n",
    "        print(f\"üì• Manual download: Right-click {package_file} in file browser\")\n",
    "    \n",
    "    print(f\"\\nüéØ Next Steps:\")\n",
    "    print(f\"   1. üì• Download completes to your computer\")\n",
    "    print(f\"   2. üìÅ Extract the ZIP file\")\n",
    "    print(f\"   3. üìã Review analytics CSV files for performance metrics\")\n",
    "    print(f\"   4. üîÑ Upload checkpoints to resume training (if not complete)\")\n",
    "    print(f\"   5. üöÄ Deploy final model to your local inference system\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Package creation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "**1. \"Required files missing\"**\n",
    "- ‚úÖ Upload all files to correct directories:\n",
    "  - `src/data_manager.py` \n",
    "  - `src/environment.py`\n",
    "  - `training/train_ppo.py`\n",
    "  - `config/training.yaml`\n",
    "\n",
    "**2. \"No GPU available\"**\n",
    "- ‚úÖ Go to Runtime ‚Üí Change Runtime Type ‚Üí Hardware Accelerator ‚Üí GPU\n",
    "- ‚úÖ Restart runtime and re-run setup cells\n",
    "\n",
    "**3. \"Training fails to start\"**\n",
    "- ‚úÖ Check all required files are uploaded correctly\n",
    "- ‚úÖ Verify Python modules import without errors\n",
    "- ‚úÖ Check error messages in training output\n",
    "\n",
    "**4. \"Colab disconnects during training\"**\n",
    "- ‚úÖ Training auto-saves every 200k steps\n",
    "- ‚úÖ Download checkpoint files after each session\n",
    "- ‚úÖ Upload checkpoints and resume next day\n",
    "\n",
    "**5. \"CSV files not loading\"**\n",
    "- ‚úÖ Upload CSV files to `/content/data/` folder\n",
    "- ‚úÖ Use exact filenames: `EURUSD_M15.csv`, `AUDCHF_M15.csv`, `USDJPY_M15.csv`\n",
    "- ‚úÖ Training will use sample data if CSV files unavailable\n",
    "\n",
    "**6. \"Poor training performance\"**\n",
    "- ‚úÖ Check TensorBoard metrics (rewards should trend upward)\n",
    "- ‚úÖ Verify win rate >60% and consistent trade execution\n",
    "- ‚úÖ Review analytics CSV files for detailed trade analysis\n",
    "\n",
    "### Support Files Format:\n",
    "\n",
    "**Expected CSV format:**\n",
    "```\n",
    "Time,Open,High,Low,Close,Volume\n",
    "2021-11-18 22:15:00,1.13714,1.1372,1.137,1.13716,438\n",
    "2021-11-18 22:30:00,1.13716,1.1375,1.1371,1.13728,502\n",
    "```\n",
    "\n",
    "**requirements_colab.txt example:**\n",
    "```\n",
    "finta==1.3\n",
    "yfinance==0.2.18\n",
    "scikit-learn==1.3.0\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}