# PPO Training Configuration for Forex Trading System
# Optimized for "$100 in 4 candles" trading goal

# Algorithm Configuration
algorithm:
  name: "PPO"
  total_timesteps: 500000  # 500k steps (12-24 hours on GPU)
  device: "cuda"  # Use GPU in Google Colab
  verbose: 1

# Model Configuration
model:
  name: "google/gemma-2b-it"  # Gemma 2B model
  policy: "MlpLstmPolicy"  # Changed from MlpPolicy - LSTM for temporal learning
  policy_kwargs:
    net_arch:
      - 512  # Hidden layer size
      - 256
      - 128
    activation_fn: "tanh"
    lstm_hidden_size: 256  # LSTM hidden units

# PPO Hyperparameters (Optimized for LSTM)
ppo_params:
  learning_rate: 0.0003      # Learning rate (3e-4) - Increase for better exploration
  n_steps: 2048              # Reduced from 4096 for LSTM (memory intensive)
  batch_size: 64             # Reduced from 128 for LSTM
  n_epochs: 10               # Epochs per update
  gamma: 0.99                # Discount factor
  gae_lambda: 0.95           # GAE lambda
  clip_range: 0.2            # PPO clipping parameter
  clip_range_vf: null        # Value function clipping
  ent_coef: 0.05             # Reduced for fresh training (was 0.15 for exploration)
  vf_coef: 1.0               # FIX: Increased from 0.5 - value function was not learning
  max_grad_norm: 0.5         # Gradient clipping
  target_kl: null            # Target KL divergence
  normalize_advantage: true

# Training Configuration - Realistic Targets
training:
  # Data splits
  validation_split: 0.2

  # Saving and logging
  save_frequency: 50000      # Save model every 50k steps (chunk intervals)
  log_frequency: 100         # Log metrics every 100 steps
  eval_frequency: 10000      # Evaluate every 10k steps

  # Directories
  model_save_path: "./models/checkpoints"
  tensorboard_log: "./logs/tensorboard"
  eval_log_path: "./logs/evaluations"
  analytics_path: "./analytics"  # Analytics export directory

  # Realistic target metrics
  target_episode_reward: 300.0    # Increased from 200 (activity rewards)
  target_win_rate: 0.42           # More realistic
  target_profit_factor: 1.6      # Gross profit / gross loss
  target_trades_per_episode: 15  # NEW: Monitor activity
  min_trades_per_episode: 5      # NEW: Minimum acceptable
  early_stopping_patience: 10    # More patience for realistic strategy
  min_reward_threshold: 50.0     # Lower minimum threshold
  max_drawdown_limit: 0.10       # Stop if >10% drawdown

# Environment Configuration - Simplified Single-Pair Strategy
environment:
  # Trading parameters - 3% risk strategy
  initial_balance: 10000.0          # Realistic retail account size
  risk_per_trade_pct: 0.03         # 3% risk per trade ($300 max loss)
  dynamic_position_sizing: true     # Enable risk-based position sizing
  max_positions_per_pair: 1
  lot_size: 0.01                   # Base lot size (will be calculated dynamically)
  max_episode_steps: 1500          # Longer episodes for patience
  max_position_duration: 24        # Max 6 hours per position (24 candles)
  commission_per_lot: 0.5

  # Currency pairs - SIMPLIFIED: Single pair only
  pairs: ["USDJPY"]

  # Spread configuration (pips)
  spread_pips:
    USDJPY: 1.8

  # Stop Loss / Take Profit (pips) - 1:1.5 risk/reward ratio
  sl_tp_pips: [15.0, 22.5]  # [stop_loss, take_profit]

# Data Configuration
data:
  # Historical data range
  history_days: 365
  timeframe: "15M"  # 15-minute candles

  # Data preprocessing
  normalize_observations: true
  handle_missing_data: "forward_fill"
  outlier_clipping: true
  outlier_std_threshold: 3.0

# Evaluation Configuration - Realistic Targets
evaluation:
  episodes: 50  # Number of evaluation episodes
  deterministic: true  # Use deterministic policy for evaluation

  # Realistic success metrics
  target_metrics:
    episode_reward: 200.0      # Realistic target (was 1000)
    win_rate: 0.45            # 45% achievable with 1:1.5 ratio
    profit_factor: 1.5        # Gross profit / gross loss
    sharpe_ratio: 0.8         # Risk-adjusted returns
    max_drawdown: 0.10        # 10% maximum drawdown

  # Specific forex metrics - Updated Strategy
  forex_metrics:
    target_profit_per_trade: 37.50   # $37.50 target (7.5% account return)
    target_duration_candles: 12      # 12 candles max (3 hours)
    successful_trades_threshold: 3   # Success if >=3 goal trades per episode
    min_profit_per_trade: 15.0      # Minimum $15 profit considered good
    max_loss_per_trade: 35.0        # Maximum $35 loss before penalty

# Callbacks Configuration
callbacks:
  # Early stopping
  early_stopping:
    enabled: true
    patience: 5
    min_reward_threshold: 500.0

  # Model checkpointing
  checkpoint:
    enabled: true
    save_freq: 25000
    save_path: "./models/checkpoints"

  # Learning rate scheduling
  lr_schedule:
    enabled: false  # Disable for initial training

  # Custom forex callback
  forex_tracker:
    enabled: true
    log_successful_goal_trades: true
    track_session_performance: true

# Resource Management
resources:
  # Memory settings
  max_memory_usage_mb: 8000

  # GPU settings
  gpu_memory_growth: true
  mixed_precision: false

  # Reproducibility
  seed: 42
  deterministic_training: true

# Export Configuration
export:
  final_model_path: "./models/final/forex_ppo_model"
  model_format: "pytorch"  # pytorch, onnx
  include_config: true
  model_version: "v1.0"

# Debug Settings
debug:
  log_level: "INFO"
  check_environment: true
  profile_training: false
  render_episodes: false  # Keep false for Colab